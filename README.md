神经网络实现
IDE : CLion 2018.1

为了方便后续实现RNN等模型和使用CUDA加速，现已重构代码至第三个版本
目前主要变化如下
1、重新设计网络和层次的抽象结构，细分层次（激活函数成为独立层次，池化层分为两个独立层次max和mean），每个层次的逻辑更简单和清晰
2、网络和层次（也包括数据集）的上层逻辑与底层算法实现分离，以便于更新、调试和使用CUDA加速
3、完善一些环境配置以及添加某些CUDA环境配置
------------------------2018.6.1------------------------
4、实现线性层的dropout（在线性层的激活后加上Dropout层）
5、修复池化层错误

要重新实现神经网络主要是因为想用显卡加速并且学习后发现RNN跟前馈网络是相容的，第一次重构时（v2）的设计是多余的
重新实现同样的功能果然是很烦的一件事，然而目前v3也只写到了这个程度
因为写的时间并不长而且很分散，目前相对v2还有数据预处理（标准化）没实现

上次提交时v3的效果差（准确率较低、收敛慢），跟v2对比测试发现问题是v3池化层一个错（脑）误（抽）写法导致反向传播时梯度在池化层丢失，导致卷积层和池化层只是降低了输入的信噪比，没有任何有效的作用。不过即便这样还有98%+的测试集准确率，可见多层感知机其实还是蛮拼的……
现在v3的性能已经和达到v2的水平了


v2已实现：
全连接神经网络
卷积神经网络（卷积层、池化层）
全连接层dropout（或许还不算严格实现，因为使用Adam或AdaMax或含L2正则化的SGD时被dropout的神经元参数仍会被修改）
输入数据预处理（normalize）

MNIST数据集的测试集准确率99%+
CIFAR-10数据集的测试集准确率最高只达到过72%左右……（或许很大程度上是因为没有gpu加速模型规模做不大）

尝试过使用OpenBLAS代替自己实现的矩阵运算，但效率反而比不上（开启O3级别优化后的）自己的实现，可能是编译OpenBLAS时没设置好……