神经网络实现
IDE : CLion 2018.1

为了方便后续实现RNN等模型和使用CUDA加速，现已重构代码至第三个版本
目前主要变化如下
1、重新设计网络和层次的抽象结构，细分层次（激活函数成为独立层次，池化层分为两个独立层次max和mean），每个层次的逻辑更简单和清晰
2、网络和层次（也包括数据集）的上层逻辑与底层算法实现分离，以便于更新、调试和使用CUDA加速
3、完善一些环境配置以及添加某些CUDA环境配置

要重新实现神经网络主要是因为想用显卡加速并且学习后发现RNN跟前馈网络是相容的，第一次重构时（v2）的设计是多余的
重新实现同样的功能果然是很烦的一件事，然而目前v3也只写到了这个程度
因为写的时间并不长而且很分散，目前连v2就有的dropout和数据预处理（标准化）都还没实现，效果也还不如v2（v3只测试了MNIST，同样的网络结构测试集准确率只有98%+，而且收敛速度慢得多），似乎也无法保证我的实现是完全正确的（包括之前的版本）……


v2已实现：
全连接神经网络
卷积神经网络（卷积层、池化层）
全连接层dropout（或许还不算严格实现，因为使用Adam或AdaMax或含L2正则化的SGD时被dropout的神经元参数仍会被修改）
输入数据预处理（normalize）

MNIST数据集的测试集准确率99%+
CIFAR-10数据集的测试集准确率最高只达到过72%左右……（或许很大程度上是因为没有gpu加速模型规模做不大）

尝试过使用OpenBLAS代替自己实现的矩阵运算，但效率反而比不上（开启O3级别优化后的）自己的实现，可能是编译OpenBLAS时没设置好……