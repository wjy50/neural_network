神经网络实现
IDE : CLion 2018.1

为了方便后续实现RNN等模型和使用CUDA加速，现已重构代码至第三个版本
要重新实现神经网络主要是因为想用显卡加速并且学习后发现RNN跟前馈网络是相容的，第一次重构时（v2）的设计是多余的
重新实现同样的功能果然是很烦的一件事

目前主要变化如下
1、重新设计网络和层次的抽象结构，细分层次（激活函数成为独立层次，池化层分为两个独立层次max和mean），每个层次的逻辑更简单和清晰
2、网络和层次（也包括数据集）的上层逻辑与底层算法实现分离，以便于更新、调试和使用CUDA加速
3、完善一些环境配置以及添加某些CUDA环境配置

------------------------2018.6.1------------------------

4、实现线性层的dropout（在线性层的激活后加上Dropout层）
5、修复池化层错误

上次提交时v3的效果差（准确率较低、收敛慢），跟v2对比测试发现问题是v3池化层一个错（脑）误（抽）写法导致反向传播时梯度在池化层丢失，导致卷积层和池化层只是降低了输入的信噪比，没有任何有效的作用。不过即便这样还有98%+的测试集准确率，可见多层感知机其实还是蛮拼的……
现在v3的性能已经和达到v2的水平了

------------------------2018.6.2------------------------

6、实现Batch Normalization层

花了差不多两天的时间终于实现了BN，期间还是碰到了不少坑。
一开始是按照论文给出的反向传播公式以及整体均值方差的估计方式实现的，结果训练效果不仅没有提升而且极其不稳定。后来自己推导反向传播公式（不确定论文给出的公式是否正确，我的推导思路跟论文的不一样，结果也是不一样的，但我按照论文的思路推导也发现了一点问题，要得到论文给出的结果，必须在求标准化后的x对输入x的偏导时将均值和方差当做常数，但我认为这是不应该的）又实现了一遍，但仍然不稳定。仔细一想觉得应该是整体均值和方差的估计有问题，再按照网上找到的用动量方式估计的方法实现，终于正常了，在CIFAR10数据集上训练收敛明显加快，也很稳定（MNIST数据集上加了BN效果反而差，应该是因为MNIST数据很稀疏，BN的标准化方法不适用）。
目前发现效果最好的加入BN的方法是在隐含层激活函数（ReLU、LeakyReLU，当然还有Sigmoid，不过我没实现隐含层的Sigmoid）前以及网络的第一层都上BN。
现在已经没什么必要实现v2那样针对数据集的标准化预处理了


v2已实现：
全连接神经网络
卷积神经网络（卷积层、池化层）
全连接层dropout（或许还不算严格实现，因为使用Adam或AdaMax或含L2正则化的SGD时被dropout的神经元参数仍会被修改）
输入数据预处理（normalize）

MNIST数据集的测试集准确率99%+
CIFAR-10数据集的测试集准确率最高只达到过72%左右……（或许很大程度上是因为没有gpu加速模型规模做不大）

尝试过使用OpenBLAS代替自己实现的矩阵运算，但效率反而比不上（开启O3级别优化后的）自己的实现，可能是编译OpenBLAS时没设置好……